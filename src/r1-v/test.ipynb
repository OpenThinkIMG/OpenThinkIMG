{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'newopenr1 (Python 3.11.11)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n newopenr1 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'newopenr1 (Python 3.11.11)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n newopenr1 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data_file = '/mnt/petrelfs/share_data/suzhaochen/chartgemma-reachqa-combined-sharegpt-filterd.json'\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example):\n",
    "    messages = []\n",
    "    \n",
    "    SYSTEM_PROMPT = (\"\"\"You are a visual assistant capable of generating and solving steps for chart-based reasoning. Your goal is to answer chart-related questions. You can rely on your own capabilities or use external tools to assist in solving. Here are the available actions:\n",
    "    - **OCR**: Extracts text from an image. Example: `{\"name\": \"OCR\", \"arguments\": {\"image\": \"img_1\"}}`\n",
    "    - **Point**: Identifies a point in the image based on description and returns coordinates. Example: `{\"name\": \"Point\", \"arguments\": {\"image\": \"img_1\", \"param\": \"x-axis value 1970\"}}`\n",
    "    - **ZoomInSubfigure**: Crops the image to the specified subfigure. Example: `{\"name\": \"ZoomInSubfigure\", \"arguments\": {\"image\": \"img_1\", \"param\": \"Downstream vs. Concept: Toy\"}}`\n",
    "    - **SegmentRegionAroundPoint**: Segments a region around a given point. Example: `{\"name\": \"SegmentRegionAroundPoint\", \"arguments\": {\"image\": \"img_1\", \"param\": \"x=\\\"21.5\\\" y=\\\"28.5\\\"\"}}`\n",
    "    - **DrawHorizontalLineByY**: Draws a horizontal line at a given y-coordinate. Example: `{\"name\": \"DrawHorizontalLineByY\", \"arguments\": {\"image\": \"img_1\", \"param\": \"y=28.5\"}}`\n",
    "    - **DrawVerticalLineByX**: Draws a vertical line at a given x-coordinate. Example: `{\"name\": \"DrawVerticalLineByX\", \"arguments\": {\"image\": \"img_1\", \"param\": \"x=21.5\"}}`\n",
    "    - **Terminate**: Ends the task and provides the final answer. Example: `{\"name\": \"Terminate\", \"arguments\": {\"ans\": \"1985\"}}`\n",
    "\n",
    "    To solve the problem:\n",
    "    1. Select actions from the provided tools list, combining them logically and building on previous steps. Call one action at a time, using its output for the next.\n",
    "    2. To use `SegmentRegionAroundPoint`, `DrawHorizontalLineByY`, or `DrawVerticalLineByX`, first call \"Point\" to get coordinates for further actions.\n",
    "\n",
    "    Your output should be in a strict JSON format as follows:\n",
    "    {\"thought\": \"the reasoning process\", \"actions\": [{\"name\": \"action\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}}]}\n",
    "    \"\"\")\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n",
    "    })\n",
    "\n",
    "    conversations = example.get('conversations')\n",
    "    image_paths = example.get('images')\n",
    "    img_idx = 0\n",
    "    \n",
    "    for item in conversations:\n",
    "        content = []\n",
    "        if item['from'] == 'human':\n",
    "            role = 'user'\n",
    "        else:\n",
    "            role = 'assistant'\n",
    "        if \"You are a visual assistant capable of generating and solving steps\" in item['value']:\n",
    "            content.append({'type':'text', 'text':item['value'].split(\"\\n\\nQuestion: \")[-1]})\n",
    "        else:\n",
    "            content.append({'type':'text', 'text':item['value']})\n",
    "        \n",
    "        if '<image>' in item['value']:\n",
    "            content.append({'type':'image', 'image':image_paths[img_idx]})\n",
    "            img_idx += 1\n",
    "        \n",
    "        messages.append({\n",
    "            'role': role,\n",
    "            'content': content\n",
    "        })\n",
    "    \n",
    "    example[\"messages\"] = messages\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/mnt/petrelfs/share_data/suzhaochen/datasets/chargemma_final/select/chartgemma-reachqa-combined-sharegpt.json'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newopenr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
