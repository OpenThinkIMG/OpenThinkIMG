{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 11:19:58 | ERROR | stderr | Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-04-25 11:20:04 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-25 11:20:39 | ERROR | stderr | /tmp/ipykernel_29427/2944994131.py:2: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "2025-04-25 11:20:39 | ERROR | stderr |   with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tool_server.utils.utils import *\n",
    "from tool_server.utils.server_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tool_server.tool_workers.online_workers.base_tool_worker import BaseToolWorker\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7896c1863c846dd8874d092b80873b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the processor\n",
    "model_path = \"/mnt/petrelfs/songmingyang/songmingyang/model/mm/tools/Molmo-7B-D-0924\"\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_prompt = \"Point to the {} in the scene.\".format(\"Point E\")\n",
    "imput_image_path = \"/mnt/petrelfs/songmingyang/code/reasoning/tool-agent/tool_server/tool_workers/online_workers/test_cases/mathvista_35.jpg\"\n",
    "image = Image.open(imput_image_path).convert(\"RGB\")\n",
    "inputs = processor.process(\n",
    "    images=[image],\n",
    "    text=text_prompt,\n",
    ")\n",
    "inputs[\"images\"] = inputs[\"images\"].to(torch.bfloat16)\n",
    "inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 576, 588])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"images\"].shape\n",
    "# inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "\n",
    "        output = model.generate_from_batch(\n",
    "            inputs,\n",
    "            GenerationConfig(max_new_tokens=1024, stop_strings=\"<|endoftext|>\"),\n",
    "            tokenizer=processor.tokenizer\n",
    "        )\n",
    "\n",
    "        # only get generated tokens; decode them to text\n",
    "        generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "        response = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <point x=\"49.8\" y=\"10.0\" alt=\"Point E\">Point E</point>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm2",
   "language": "python",
   "name": "vllm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
