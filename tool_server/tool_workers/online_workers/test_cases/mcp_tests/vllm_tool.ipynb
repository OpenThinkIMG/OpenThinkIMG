{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ad4cef-ec48-49a8-bdba-c7a50a03fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 16:31:18 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f705a58-d9d3-4def-a362-0461f379bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 16:31:39 [config.py:585] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-02 16:31:39 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-02 16:31:41 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/mnt/petrelfs/songmingyang/songmingyang/model/mm/Qwen2.5-VL-3B-Instruct', speculative_config=None, tokenizer='/mnt/petrelfs/songmingyang/songmingyang/model/mm/Qwen2.5-VL-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/mnt/petrelfs/songmingyang/songmingyang/model/mm/Qwen2.5-VL-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-02 16:31:42 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa51eedb4f0>\n",
      "INFO 06-02 16:31:43 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-02 16:31:43 [cuda.py:220] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 16:31:43 [gpu_model_runner.py:1174] Starting to load model /mnt/petrelfs/songmingyang/songmingyang/model/mm/Qwen2.5-VL-3B-Instruct...\n",
      "INFO 06-02 16:31:44 [config.py:3243] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "WARNING 06-02 16:31:44 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe36d5906d14eb4b60f00a8e0e9f637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 16:31:54 [loader.py:447] Loading weights took 9.47 seconds\n",
      "INFO 06-02 16:31:54 [gpu_model_runner.py:1186] Model loading took 7.1557 GB and 10.179992 seconds\n",
      "INFO 06-02 16:31:54 [gpu_model_runner.py:1456] Encoder cache will be initialized with a budget of 98304 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "INFO 06-02 16:32:15 [backends.py:415] Using cache directory: /mnt/petrelfs/songmingyang/.cache/vllm/torch_compile_cache/ab008d54b6/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-02 16:32:15 [backends.py:425] Dynamo bytecode transform time: 10.71 s\n",
      "INFO 06-02 16:32:16 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 06-02 16:32:26 [monitor.py:33] torch.compile takes 10.71 s in total\n",
      "INFO 06-02 16:32:27 [kv_cache_utils.py:566] GPU KV cache size: 1,679,280 tokens\n",
      "INFO 06-02 16:32:27 [kv_cache_utils.py:569] Maximum concurrency for 128,000 tokens per request: 13.12x\n",
      "INFO 06-02 16:32:49 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 1.96 GiB\n",
      "INFO 06-02 16:32:49 [core.py:151] init engine (profile, create kv cache, warmup model) took 55.16 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/mnt/petrelfs/songmingyang/songmingyang/model/mm/Qwen2.5-VL-3B-Instruct\"\n",
    "# or switch to \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "# or \"mistralai/Mistral-Large-Instruct-2407\"\n",
    "# or any other mistral model with function calling ability\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=8192, temperature=0.0)\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbd6e91-59f6-41cb-9294-77e6c8c09722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s, est. speed input: 1595.26 toks/s, output: 126.72 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<tool_call>\\n{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}}\\n</tool_call>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generate_random_id(length=9):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    random_id = \"\".join(random.choice(characters) for _ in range(length))\n",
    "    return random_id\n",
    "\n",
    "\n",
    "# simulate an API that can be called\n",
    "def get_current_weather(city: str, state: str, unit: \"str\"):\n",
    "    return (\n",
    "        f\"The weather in {city}, {state} is 85 degrees {unit}. It is \"\n",
    "        \"partly cloudly, with highs in the 90's.\"\n",
    "    )\n",
    "\n",
    "\n",
    "tool_functions = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                    },\n",
    "                    \"state\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                        \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unit to fetch the temperature in\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"city\", \"state\", \"unit\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather2\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                    },\n",
    "                    \"state\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                        \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unit to fetch the temperature in\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"city\", \"state\", \"unit\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params, tools=tools)\n",
    "output = outputs[0].outputs[0].text.strip()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24f9f2a-ea13-427b-871a-940f79c4e007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str = output.split(\"<tool_call>\")[-1].split(\"</tool_call>\")[0].strip()\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41bd369-0a25-4047-a8b2-a5e93509fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s, est. speed input: 2161.69 toks/s, output: 124.78 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dallas, TX is 85 degrees Fahrenheit. The weather is partly cloudy, with expected highs in the 90's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# append the assistant message\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": output,\n",
    "    }\n",
    ")\n",
    "\n",
    "# let's now actually parse and execute the model's output simulating an API call by using the\n",
    "# above defined function\n",
    "json_str = output.split(\"<tool_call>\")[-1].split(\"</tool_call>\")[0].strip()\n",
    "tool_calls = json.loads(json_str)\n",
    "tool_answers = [\n",
    "    tool_functions[tool_calls[\"name\"]](**tool_calls[\"arguments\"]) for call in tool_calls\n",
    "]\n",
    "\n",
    "# append the answer as a tool message and let the LLM give you an answer\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": \"\\n\\n\".join(tool_answers),\n",
    "        \"tool_call_id\": generate_random_id(),\n",
    "    }\n",
    ")\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params, tools=tools)\n",
    "\n",
    "print(outputs[0].outputs[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066fe22e-9ddd-4189-8c85-2eeece2a8736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The weather in Dallas, TX is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's.\",\n",
       " \"The weather in Dallas, TX is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a532e4-c86d-44f8-99ea-4d4a81522598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_current_weather',\n",
       " 'arguments': {'city': 'Dallas', 'state': 'TX', 'unit': 'fahrenheit'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca012918-fe2e-4e54-8a4e-a793d4484578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=4, prompt='<|im_start|>system\\nYou are a helpful assistant.\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"The city to find the weather for, e.g. \\'San Francisco\\'\"}, \"state\": {\"type\": \"string\", \"description\": \"the two-letter abbreviation for the state that the city is in, e.g. \\'CA\\' which would mean \\'California\\'\"}, \"unit\": {\"type\": \"string\", \"description\": \"The unit to fetch the temperature in\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"city\", \"state\", \"unit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather2\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"The city to find the weather for, e.g. \\'San Francisco\\'\"}, \"state\": {\"type\": \"string\", \"description\": \"the two-letter abbreviation for the state that the city is in, e.g. \\'CA\\' which would mean \\'California\\'\"}, \"unit\": {\"type\": \"string\", \"description\": \"The unit to fetch the temperature in\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"city\", \"state\", \"unit\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nCan you tell me what the temperate will be in Dallas, in fahrenheit?<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n<tool_response>\\nThe weather in Dallas, TX is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90\\'s.\\n\\nThe weather in Dallas, TX is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90\\'s.\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 382, 2, 13852, 271, 2610, 1231, 1618, 825, 476, 803, 5746, 311, 7789, 448, 279, 1196, 3239, 382, 2610, 525, 3897, 448, 729, 32628, 2878, 366, 15918, 1472, 15918, 29, 11874, 9492, 510, 27, 15918, 397, 4913, 1313, 788, 330, 1688, 497, 330, 1688, 788, 5212, 606, 788, 330, 455, 11080, 69364, 497, 330, 4684, 788, 330, 1949, 279, 1482, 9104, 304, 264, 2661, 3728, 497, 330, 13786, 788, 5212, 1313, 788, 330, 1700, 497, 330, 13193, 788, 5212, 8926, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 785, 3283, 311, 1477, 279, 9104, 369, 11, 384, 1302, 13, 364, 23729, 12879, 6, 14345, 330, 2454, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 1782, 1378, 79368, 71478, 369, 279, 1584, 429, 279, 3283, 374, 304, 11, 384, 1302, 13, 364, 5049, 6, 892, 1035, 3076, 364, 45410, 6, 14345, 330, 3843, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 785, 4982, 311, 7807, 279, 9315, 304, 497, 330, 9018, 788, 4383, 66, 40247, 497, 330, 69, 47910, 1341, 38154, 330, 6279, 788, 4383, 8926, 497, 330, 2454, 497, 330, 3843, 1341, 3417, 532, 4913, 1313, 788, 330, 1688, 497, 330, 1688, 788, 5212, 606, 788, 330, 455, 11080, 69364, 17, 497, 330, 4684, 788, 330, 1949, 279, 1482, 9104, 304, 264, 2661, 3728, 497, 330, 13786, 788, 5212, 1313, 788, 330, 1700, 497, 330, 13193, 788, 5212, 8926, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 785, 3283, 311, 1477, 279, 9104, 369, 11, 384, 1302, 13, 364, 23729, 12879, 6, 14345, 330, 2454, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 1782, 1378, 79368, 71478, 369, 279, 1584, 429, 279, 3283, 374, 304, 11, 384, 1302, 13, 364, 5049, 6, 892, 1035, 3076, 364, 45410, 6, 14345, 330, 3843, 788, 5212, 1313, 788, 330, 917, 497, 330, 4684, 788, 330, 785, 4982, 311, 7807, 279, 9315, 304, 497, 330, 9018, 788, 4383, 66, 40247, 497, 330, 69, 47910, 1341, 38154, 330, 6279, 788, 4383, 8926, 497, 330, 2454, 497, 330, 3843, 1341, 3417, 532, 522, 15918, 1339, 2461, 1817, 729, 1618, 11, 470, 264, 2951, 1633, 448, 729, 829, 323, 5977, 2878, 220, 151657, 151658, 11874, 9492, 510, 151657, 198, 4913, 606, 788, 366, 1688, 11494, 8066, 330, 16370, 788, 366, 2116, 56080, 40432, 31296, 151658, 151645, 198, 151644, 872, 198, 6713, 498, 3291, 752, 1128, 279, 6797, 349, 686, 387, 304, 18542, 11, 304, 282, 47910, 30, 151645, 198, 151644, 77091, 198, 151657, 198, 4913, 606, 788, 330, 455, 11080, 69364, 497, 330, 16370, 788, 5212, 8926, 788, 330, 86514, 497, 330, 2454, 788, 330, 22867, 497, 330, 3843, 788, 330, 69, 47910, 95642, 151658, 151645, 198, 151644, 872, 198, 27, 14172, 9655, 397, 785, 9104, 304, 18542, 11, 17031, 374, 220, 23, 20, 12348, 282, 47910, 13, 1084, 374, 27037, 9437, 398, 11, 448, 53779, 304, 279, 220, 24, 15, 594, 382, 785, 9104, 304, 18542, 11, 17031, 374, 220, 23, 20, 12348, 282, 47910, 13, 1084, 374, 27037, 9437, 398, 11, 448, 53779, 304, 279, 220, 24, 15, 594, 624, 522, 14172, 9655, 29, 151645, 198, 151644, 77091, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"The current temperature in Dallas, TX is 85 degrees Fahrenheit. The weather is partly cloudy, with expected highs in the 90's.\", token_ids=[785, 1482, 9315, 304, 18542, 11, 17031, 374, 220, 23, 20, 12348, 68723, 13, 576, 9104, 374, 27037, 73549, 11, 448, 3601, 53779, 304, 279, 220, 24, 15, 594, 13, 151645], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58343f9-181f-4b96-bddf-a63104191c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm2",
   "language": "python",
   "name": "vllm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
